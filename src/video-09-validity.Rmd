---
title: "Video 9 - Validity and reliability"
author: "Steve Simon"
output: 
  powerpoint_presentation:
    reference_doc: ../doc/template.pptx
    slide_level: 3
---

```{r echo=FALSE}
source("prelims.R", echo=FALSE)
```

***
`r read_text("objectives09")`

<div class="notes">

Here are the objectives for this week.

</div>

***
`r read_text("readings09")`

<div class="notes">

This is what you should have read already. If you haven't done the reading yet, pause this video and read this material. You'll get more out of the video if you do so.

</div>

### Three types of validity

* Internal validity
  + "The extent to which we can infer that the independent variable caused the dependent variable."
* External validity
  + "The extent to which the findings will generalize to other populations, settings, measures, and treatments."
* Measurement validity
  + "The quality of accuracy of individual measures or scores. The extent to which a score measures what it was intended to measure."  

+ "Study quality also depends on the *consistency* (measurement reliability) and *accuracy* (measurement validity) of the specific instruments"

<div class="notes">

The two articles Green et al, Ratliff et al will be the focus of discussion activities. Conistent and free from error, reproducible. It is measuring what it is intended to measure.

</div>

***
***
### Validity 

![](../images/image-07-01.png)

<div class="notes">

Figure 8.1 has a very concise presentation. Three different types of validity that feed into what a study does. This lecture is just about internal validity. The extent to which you can infer that the independent variable caused the dependent variable.

</div>

***
### Internal Validity 

+ “The extent to which we can infer that the independent variable *caused* the dependent variable.”
	+ For non-experimental studies –
		+ How “well designed and conducted” was the study?
+ Three criteria for causality

<div class="notes">

It's really only randomized trials that can establish causality. But even in quasi-experimental and non-experimental studies, you can still talk about internal validity.

The cause has to precede the effect, it must be connected to the effect, and there must be no other variables that could explain why the cause is related to the effect.

</div>

***
### Internal Validity 

+ “The extent to which we can infer that the independent variable *caused* the dependent variable.”
+ Three criteria for causality
	+ IV *must precede* the outcome variable
	+ IV *must be related* to the outcome
	+ There must be no other variables that could explain why the IV is related to the outcome

<div class="notes">



</div>

***
### Internal Validity 

+ Three criteria for causality –
	+ IV *must precede* the outcome variable
	+ IV *must be related* to the outcome
	+ There must be no other variables that could explain why the IV is related to the outcome
+ By Research Approach –
	+ Randomized Exp
	+ Quasi- Exp
	+ Comparative
	+ Associational
	+ Descriptive

<div class="notes">



</div>

***
### Internal Validity 

```{r tbl5, echo = FALSE, eval=FALSE}
tbl5 <- tibble::tribble(
~`Criteria for Causality`, ~`Randomized Exp`, ~`Quasi-Exp`, ~`Comparative`, ~`Associational`, ~`Descriptive`,
"IV must precede the outcome variable","Met","Often met","Sometimes met, but order might not be clear","Sometimes met, but order might not be clear","NA",
"IV must be related to the outcome","Met","Usually met","Usually met","Usually met","NA",
"There must be no other variables that could explain why the IV is related to the outcome","Met","Met in the strongest designs","Not possible","Not possible","NA"
)

kableExtra::kable_styling(knitr::kable(tbl5), font_size = 18)
```

<div class="notes">

For randomized designs, the cause precedes the effect, the cause is related to the outcome and there are no other variables that could explain the relationship.

</div>

***
### Internal Validity 

+ Meeting the 3 causality criteria based on –
	+ Strength of the research design
	+ Internal validity

<div class="notes">

The strength of the research design and the internal validity determine whether you can claim causality. Try to design the study to maximize internal validity. 

</div>

***
### Internal Validity 

+ Internal validity -
	+ Most often discussed with reference to randomized experimental and quasi-experimental designs
	+ Can also be applied to non-experimental studies

<div class="notes">



</div>

***
### Internal Validity 

+ Evaluating the internal validity of a study –
	+ Equivalence of the groups on participant characteristics
	+ Control of extraneous experiences and environmental variables

<div class="notes">

There are two aspects of internal validity, equivalence and control.

</div>

***
### Internal Validity 

+ Equivalence of the groups on participant characteristics –
	+ Are groups equivalent prior to introduction of IV?
		+ Randomized experimental design –
			+ Random assignment
		+ Quasi-experimental design –
			+ Random assignment of treatments
			+ Matching
			+ Checking pretest scores

<div class="notes">

Random assignment assures equivalence of the two groups on average. A rule of thumb is 30 people assigned to each group should give you confidence that random assignment will assure equivalence. If you are using random assignment, you should not need to test for baseline equivalence. We feel that random assignment is the best way to achieve equivalence.

In quasi-experimental design, you don't have random assignment, but you can randomly assign between the two groups. You can also use matching to achieve equivalence. Finally, you can check baseline scores for equivalence.

</div>

***
### Internal Validity 

+ Equivalence of the groups on participant characteristics –
	+ Are groups equivalent prior to introduction of IV?
		+ Comparative design –
			+ Statistical adjustment (ANCOVA) to adjust DV scores to make groups more nearly equivalent
			+ Matching participants on variables other than the primary IV
				+ E.g. Case-control study
			+ Check after the study for comparability

<div class="notes">

Since non-randomized trials have an attribute variable, you need to rely on statistical adjustment or matching. You can also check after the study for comparability.

</div>

***
### Internal Validity 

+ Equivalence of the groups on participant characteristics –
	+ Are groups equivalent prior to introduction of IV?
		+ Associational design –
			+ Only 1 group
			+ Not able to provide evidence of causation
			+ “Equivalence” – “… whether those who score high on the IV … are similar to those … who score low in terms of other attributes that may be related to the DV.”
			+ May be able to provide some statistical control

<div class="notes">

Equivalence when you have a continuous independent variable means that those who score high on the variable are similar to those who score low.

</div>

***
### Internal Validity 

+ Control of extraneous experiences and environmental variables –
	+ Extraneous variables – variables other than the IV and DV
	+ Environmental variables – conditions/variables that occur during the study
	+ Contamination
	+ Issue – Is one group affected more than the other(s)?
	+ Less of an issue with a more controlled research setting

<div class="notes">

Your study could be contaminated by other variables. These variables might be out of your control but which can influence the outcome.

Contamination: people in the intervention group are friends with the control group and share information.

The key issue is whether one group is more affected by extraneous variables.

In a controlled setting, there are fewer extraneous variables, but this changes in a field setting.

</div>

***
### Internal Validity 

+ Rating the dimensions of internal validity
	+ Figure 8.2
	+ Evaluating Research Validity framework
	+ “Good” study – moderate to high internal validity on both dimensions

<div class="notes">

Internal validity is measured by whether the two dimensions are addressed.

</div>

***
### Internal Validity 

![Rating the dimensions of internal validity](../images/image-07-02.png)

<div class="notes">

Figure 8.2. 

</div>

***
### Threats to Internal Validity 

![](../images/image-07-03.png)

<div class="notes">

Table 8.1 relates statistical terminology to identify threats to internal validity. This table tries to take those technical terms and express them 

Extreme groups have issues with regression to the mean. Even without any intervention, the extremities will tend to lessen.

Dropouts or attrition means that you've designed a setting that is so difficult that no one can stay in. Differential attrition is especially troublesome.

Bias in assignment occurs when patients or their physicians directly or indirectly influence the assignment. Random assignment eliminates this bias.

Cook and Campbell.

</div>

***
### Threats to Internal Validity 

+ Equivalence of Groups
	+ Use of extreme groups
	+ Participant dropouts or attrition during the study
	+ Bias in assignment to groups

<div class="notes">



</div>

***
### Threats to Internal Validity 

+ Control of extraneous/environmental variables
	+ Changes due to time or growth and development
	+ Extraneous environmental events
	+ Repeated testing, carryover effects
	+ Instrument or observer inconsistency
	+ Combinations of two or more threats
	+ Did the IV actually occur before the DV?

<div class="notes">

Here are some of the sources of extraneous variables. 

Maturation.

History. Something that occurs between the pre and post measurements that is independent of the intervention but which can influence the outcome.

Worry about sensitization to the issue in the baseline measurement that causes people to go out and find more.

Long studies have problems with instrumentation or observers (e.g., because of turnover).

There are also combinations of these events where one magnifies the other.

Temporal order of the variables. If you can't assure that the independent variable occurs before the dependent variable, you can't establish causation.

</div>

***
### Threats to Internal Validity 

+ Other threats
	+ Effects of being in the control group
	+ Expectation effect
		+ Control for expectation
	+ Observer / experimenter bias

<div class="notes">

Control group contamination. Demoralization.

Hawthorne effect.

Blinding helps control for expectation effects and observer bias.

</div>

***
### Validity 

![](../images/image-07-04.png)

<div class="notes">

Here is figure 8.1 again. The next part of this lecture talks about sampling and external validity.

</div>

***
### Validity 

![](../images/image-07-05.png)

<div class="notes">

Here is figure 8.1 again. The extent to which results will generalize.

</div>

### Measurement Reliability 

+ What is measurement reliability?
	+ “… consistency of a series of measurements. ( Cronbach )
	+ “… a property of scores and is not immutable across all conceivable uses of a given measure.” (Thompson)
+ Importance – without reliable measures, can’t have confidence in study results.

<div class="notes">

We can expect consistent responses. It is able to produce with predictable consistency over time. Without reliability, you have to worry about what the study is telling you.

Suppose you are measuring change from baseline. Unless the scores change in a predictable way. Otherwise, you won't know if the changes are related to the intervention or if they represent random variation.

</div>

***
### Measurement Reliability 

+ Example – Want to determine change in some relevant measure after treatment/intervention.
	+ Baseline measure
	+ Treatment
	+ Follow-up measure
+ C hange from baseline to follow-up –
	+ Due to effect of treatment?
	+ Due to random variation in the measure?

<div class="notes">



</div>

***
### Measurement Reliability 

+ Observe score
	+ Lab value
	+ Test result
	+ Self-report measure
+ Classical test theory
	+ Observed score = True score + Error
+ Change in value – due to change in true score or error?

<div class="notes">

We don't expect the observed scores to have perfect reliability. There is some fallibility. Something random can occur and people are not always consistent in how they answer things. Measurement error (noise) is messing things up. You want the observed score to reflect the true score more than error.

Systematic error move in a consistent direction. You can often correct for this using calibration. It is often not a problem for reliability, but it will affect validity.

Random errors are errors due to chance. They have unpredictable effects. They usually occur due to unpredictable factors. Less random error means that the observed score more truly affects the underlying true score.

Errors could be due to observer making an error. It is really important to follow established protocols.

Resting heart rate could be influenced by a loud noise, for example. You would hope that this occurs randomly.

</div>

***
### Measurement Reliability 

+ Goal – Use measure that
	+ Best reflects the true score
	+ As little “error” as possible
	+ As “reliable” as possible
+ Measurement reliability –
	+ Coefficient
	+ Ratio – True score / Observed score

<div class="notes">

The reliability coefficient is a ratio of the true score to the observed score.

</div>

***
### Measurement Reliability 

+ How much confidence can we have that the measure we obtain reflects the true score?
	+ Reliability of the measure
	+ How variable is the measure
+ Standard error of measurement –
	+ Allows you to “… establish a range of scores (i.e., confidence interval) within which should lie … true score.”

<div class="notes">

Suppose you want to get at a person's level of functioning. If the functioning does not change, then the measure does not change. When functioning does change, you want to the measure to change as well.

The standard error places bounds on where the true score lies.

</div>

***
### Measurement Reliability 

+ Standard error of measurement –
	+ SEM = SD * SqRt (1 – r)
		+ SEM – Standard error of measurement
		+ SD – standard deviation
		+ r – correlation coefficient
			+ r – indication of the relationship between 2 measures

<div class="notes">



</div>

***
### Measurement Reliability 

+ In order to know the range of true scores that are indicated by the observed score –
	+ Need the SEM to compute a confidence interval around the observed score
	+ Confidence interval –
		+ U sually 95% CI (2 standard deviations)
		+ To have 95% confidence that the range will include the true score

<div class="notes">



</div>

***
### Measurement Reliability 

+ Example – Intelligence test #1
	+ SD = 15; r = .92
	+ SEM = 4.24
	+ Observed score = 110
	+ 95% CI <U+F0E8> SEM * 1.96 (z score for 2 SD)
	+ 95% CI <U+F0E8> Observed +/- SEM * 1.96
	+ 95% CI <U+F0E8> 101.68 – 118.32
	+ Range of scores that includes the true score given 95% CI

<div class="notes">

r is the reliability coefficient.

</div>

***
### Measurement Reliability 

+ Example – Intelligence test #2
	+ SD = 15; r = .65 (less reliable measure)
	+ SEM = 8.87
	+ Observed score = 110
	+ 95% CI <U+F0E8> SEM * 1.96 (z score for 2 SD)
	+ 95% CI <U+F0E8> Observed +/- SEM * 1.96
	+ 95% CI <U+F0E8> 92.61 – 127.61
	+ Range of scores that includes the true score given 95% CI

<div class="notes">

If the reliability is lower, then the range of scores is much wider.

</div>

***
### Measurement Reliability 

+ Correlation coefficient
	+ For measurement reliability, general rules –
		+ “reliable”	.7 to 1.0
	+ More strict criteria
		+ > = .90	for measures used to make decisions about individuals
		+ .80		acceptable for research
		+ .70 - .80	somewhat lower than desirable
	+ In practice
		+ Measures used with reliability of .60 and higher

<div class="notes">

It's not unusual to see reliabilities as low as 0.7 or even 0.6.

</div>

***
### Measurement Reliability 

+ Test-retest
+ Parallel forms
+ Internal consistency
	+ Split-half
	+ Kuder -Richardson 20
	+ Cronbach’s alpha
+ Interrater
	+ Percentage agreement methods
	+ Intraclass correlation coefficients
	+ Interrater

<div class="notes">

Reliability is usually established when a measure is developed. When you go about using a measure, look at what's already been published. Is it used in a context similar to yours. Was the time frame for test-retest reliability comparable to your time frames?

Test-retest reliability is the coefficent of stability (repeatability). This only makes sense in settings where you expect things to remain stable.

Intelligence is likely to be stable along long time frames but mood changes rapidly.

What size interval would you expect the measure to be stable?

Parallel forms, alternate forms. If you are concerned that information known at baseline, just that process of collecting that data might have an influence. You can get at this with using different items, reordering items.

Homogeneity of scale. The extent to which the items are measuring the same underlying construct.

Split half, compare first half versus second half, odd items versus even items, random items. It is not necessarily easy to do because you have to believe that the two items are comparable. Also when you reduce the number of items, this influences the reliability. The Spearman-Brown formula adjusts for this.

</div>

***
### Measurement Reliability 

+ Measurement reliability
	+ Generally established when a measure is developed
+ For your study
	+ Check past reliability of measure
	+ If test-retest, is time period comparable?
	+ Is sample comparable?
	+ Report both established reliability AND reliability coefficient found in your study

<div class="notes">



</div>

***
### Measurement Reliability 

+ Test-retest
+ Parallel forms
+ Internal consistency
	+ Split-half
	+ Kuder -Richardson 20
	+ Cronbach’s alpha
+ Interrater
	+ Percentage agreement methods
	+ Intraclass correlation coefficients
	+ Interrater

<div class="notes">



</div>

***
### Measurement Reliability 

+ Test – retest reliability
	+ Coefficient of stability ( Cronbach )
	+ Test score will be stable if measured at different time points
		+ Timing of testing interval is critical
	+ Test-retest reliability – previously established
	+ Concern if reliability is < .70

<div class="notes">

The average of all possible split half items. It is used with interval response scale.

</div>

***
### Measurement Reliability 

+ Test-retest
+ Parallel forms
+ Internal consistency
	+ Split-half
	+ Kuder -Richardson 20
	+ Cronbach’s alpha
+ Interrater
	+ Percentage agreement methods
	+ Intraclass correlation coefficients
	+ Interrater

<div class="notes">

Before you go into this assess the intra-rater reliability. Consistency of a single rater.

Look at percent agreement. How many different events are you going to have raters code for.

</div>

***
### Measurement Reliability 

+ Parallel forms reliability
	+ Addresses concerns about “testing” or “carryover” effects
	+ Second or parallel form
		+ Reordering the items
		+ New items
	+ Reliability coefficient of at least .80 is desirable

<div class="notes">



</div>

***
### Measurement Reliability 

+ Test-retest
+ Parallel forms
+ Internal consistency
	+ Split-half
	+ Kuder -Richardson 20
	+ Cronbach’s alpha
+ Interrater
	+ Percentage agreement methods
	+ Intraclass correlation coefficients
	+ Interrater

<div class="notes">



</div>

***
### Measurement Reliability 

+ Internal consistency – Split-Half method
	+ Correlate 2 halves of the same test/survey
		+ 1 st half vs 2 nd half
		+ Odd items vs even items
		+ Random sample of half the items vs the other half
	+ Issue – reducing ###of items
		+ Reduces strength of relationship
		+ Underestimate reliability
		+ Adjust using the Spearman-Brown formula

<div class="notes">



</div>

***
### Measurement Reliability 

+ Internal consistency – Kuder -Richardson 20
	+ Measures inter-item reliability
	+ Assuming instrument measures a single trait/construct
	+ Dichotomous item score

<div class="notes">



</div>

***
### Measurement Reliability 

+ Internal consistency – Cronbach’s Alpha
	+ Measures inter-item reliability
	+ Assuming instrument measures a single trait/construct
	+ Interval response scale
	+ Alpha values .70 and higher - acceptable

<div class="notes">



</div>

***
### Measurement Reliability 

+ Test-retest
+ Parallel forms
+ Internal consistency
	+ Split-half
	+ Kuder -Richardson 20
	+ Cronbach’s alpha
+ Interrater
	+ Percentage agreement methods
	+ Intraclass correlation coefficients
	+ Interrater

<div class="notes">



</div>

***
### Measurement Reliability 

+ Interrater Reliability – Percentage Agreement
	+ Two or more raters
	+ Agreement on what will be rated
	+ Each person rates independently
	+ Compute ratio of agreement
	+ Issue –
		+ Agreement on ###of events
		+ Agreement on when events occurred?

<div class="notes">



</div>

***
### Measurement Reliability 

+ Interrater Reliability – Intraclass Correlation Coefficients (ICCs)
	+ Calculate reliability coefficient for more than 2 raters
	+ Must have interval response scale
	+ Computation – ANOVA with repeated measures
		+ How related are the rating by different raters?

<div class="notes">



</div>

***
### Measurement Reliability 

+ Interrater Reliability – Kappa
	+ Calculate reliability coefficient for 2 or more  raters
	+ Nominal data
	+ Compute agreement between 2 raters, taking into account “ marginals ”

<div class="notes">

Kappa goes into more than agreement. Compares agreement to the amount expected by chance.

</div>

***
### Measurement Reliability 

![](../images/image-09-01.png)

<div class="notes">

From the textbook. Table 11.1. Some get at participant responses. Some get at observer responses.

</div>

***
### Measurement Reliability 

+ Theories
	+ Classical test theory
		+ Observed = True score + random error
	+ Generalizability theory
		+ Different components of error
			+ Random error
			+ Test-retest error
			+ Rater error
			+ Other identifiable sources of error
	+ Item response theory
		+ Separate test characteristics from participant characteristics

<div class="notes">

Generalizability theory gets at reliability differently from classical test theory. It looks at facets of reliability.

No one coefficient provides a complete assessment of reliability.

The third theory is item response theory separates test itmes from participant characteristics. All items are not equally revealing.

IRT useful for intelligence tests.

</div>

***
### Measurement Validity 

+ “… establishing evidence for the use of a particular measure or instrument in a particular setting with a particular population for a specific purpose.”
+ Providing evidence for validity
	+ NOT “test is valid” or “test is invalid”
+ MUST have reliable measure before you can have validity
	+ May have reliability without validity

<div class="notes">

When we develop measures it is with respect to discrimination, evaluation, or prediction. What is the measure intended for? Establishing validity isn't as simple as establishing validity.

Validity implies that the measure is relatively free from error.

</div>

***
### Measurement Validity 

+ Evidence of validity reported when measure is developed
	+ Not routinely reported in research reports when measure is used
+ Types of evidence for validity –
	+ Content
	+ Response processes
	+ Internal structure
	+ Relations to other variables
	+ Consequences

<div class="notes">

You are really interested in reflecting the underlying construct. If the hypothesis is not supported, could it be the measurement at fault?

You don't see ongoing evidence of validity. What is already out in the literature.

if you are using a measure in a very different context. For instance, if nobody has used this measure in a population, you might want a second measure that has been established.

</div>

***
### Measurement Validity 

+ Content evidence –
	+ “… whether the content that makes up the instrument is representative of the concept that one is attempting to measure.”
	+ Include major aspects of the concept
	+ Does not include irrelevant material

<div class="notes">

Is the content of the evidence is consistent with what you expect.

[[Face validity versus content validity]]

It represents model. Not influenced by things like literacy factors.

This is determined subjectively by a panel of experts. Needs to be an agreement and common understanding.

</div>

***
### Measurement Validity 

+ Content evidence –
	+ Definition of the concept
	+ Literature search to determine how concept has been measured previously
	+ Generate items to represent concept
	+ Use experts to reduce items to a final set to represent the concept

<div class="notes">



</div>

***
### Measurement Validity 

+ Response processes evidence –
	+ "the extent to which the types of participant responses match the intended construct."
		+ NOT socially desirable responses
		+ NOT "test-taking" skills
	+ Observe respondents as they complete measure
	+ Question respondents about reasons for responses
	+ Also, observation of raters / judges

<div class="notes">



</div>

***
### Measurement Validity 

+ Internal structure evidence –
	+ “Evidence from several types of analysis, including factor analysis and differential item functioning …”
	+ Does an analysis of the internal structure of a measure match the conceptual framework?

<div class="notes">

Factor analysis is a method of grouping items representative of individual constructs.

In the week 9 folder is the MOTIV* document. The CES-D score has twenty items rated on a 0 to 4 scale, with four of the items reverse scaled. 

There is a somatic factor in this scale. This might be considered a subscale. Run a factor analysis, even though it only has four levels. Do the items all hang together.



</div>

***
### Measurement Validity 

+ Factor analysis – Beliefs about ART measure
+ The following questions involve your personal views about the HIV medications that have been prescribed for you.  Please indicate the extent to which you agree or disagree with the following statements.
+ Response scale: 1 (strongly disagree), 2 (disagree), 3 (uncertain), 4 (agree), 5 (strongly agree)

```{r tbl34, echo = FALSE, eval=FALSE}
tbl34 <- tibble::tribble(
~``,
"(C) b. Having to take medicines worries me",
"(N) c. My life would be impossible without my medicines",
"(N) d. Without my medicines I would be very ill",
"(C) e. I sometimes worry about long-term effects of my medicines",
"(C) f. My medicines are a mystery to me",
"(N) g. My health in the future will depend on my medicines",
"(C) h. My medicines disrupt my life",
"(C) i.  I sometimes worry about becoming too dependent on my medicines",
"(N) j.  My medicines protect me from becoming worse"
)

kableExtra::kable_styling(knitr::kable(tbl34), font_size = 18)
```

<div class="notes">

The ones in yellow. Multiple items to represent a measure. Is there variability in any of those items. If you had selected one and only one item, it might be influenced by external factors like your mood. Multiple items avoid this problem.

The N items had Cronbach's alpha of ?? and the C items had Cronbach's alpha of ??.

A factor analysis allows us to see if the measure is behaving the way we expect it to.

</div>

***
### Measurement Validity 

+ Relations to other variables evidence –
	+ Are there relations with other measures that would be predicted from the theoretical framework of the measure?
	+ Test-criterion
		+ Predictive-criterion
		+ Concurrent-criterion
	+ Convergent
	+ Discriminant
	+ Validity generalization

<div class="notes">

Predictive: can your measure be a valid predictor of some future event. Example: does ACT score predict college GPA. There are practical limitations because the outcome measure needs to be obtained for everyone.

Concurrent: taken at about the same time, new measure versus an established one.

Convergent: Multiple measures behave similarly.

Discriminant: fail to find a relationship where there shouldn't be a relationship.

Generalization: The extent to which it can be generalized to a new situation.

</div>

***
### Measurement Validity 

+ Motivation/Readiness/Confidence to Adhere

![](../images/image-09-02.png)

<div class="notes">

Motivation/readiness/confidence to adhere. Look at predictive validity versus how well patients adhere. Test for concurrent validity. Different for people in different stages of the "stages of change".

Concurrent: self-efficacy, autonomy, locus of control.

Discriminant validity: not related to education.

</div>

***
### Measurement Validity 

+ Consequences evidence –
	+ “… includes both positive and negative anticipated and unanticipated consequences of measurement.”
	+ How do the use of measures affect respondents?

<div class="notes">



</div>

***
### Measurement Validity 

+ Evaluation of measurement validity
	+ For content, response process, internal structure, and consequence –
		+ Subjective, depends on logical judgment by researcher
	+ Relations with other variables –
		+ Often correlations
		+ Judgment; no established cut-offs

<div class="notes">

For four of these, it is largely a subjective assessment. Others involve relations (often correlations). There is no official dividing line for these correlations to establish valid versus not valid.

</div>

***
### Measurement Validity 

+ Evaluation of measurement validity
	+ Cohen’s guidelines – strength of relationship
	+ Correlation coefficient – most common
	+ Applied behavioral sciences
		+ r >= .5 <U+F0E8> large effect / strong support
		+ r > .3 <U+F0E8> acceptable level of support
		+ r > .1 <U+F0E8> weak support (if statistically significant)
	+ Table 12.2

<div class="notes">



</div>

***
### Measurement Validity 

![](../images/image-08-03.png)

<div class="notes">

Table 12.2 explains what each type of validity means.

</div>

***
### Measurement Validity 

+ Validity of diagnostic tests
+ Sensitivity
	+ A test's ability to obtain a positive result when the target condition is really present
		+ True positive rate
+ Specificity
	+ A test's ability to obtain a negative result when the target condition is really absent
		+ True negative rate

<div class="notes">



</div>

***
### Measurement Validity 

![](../images/image-08-04.png)

<div class="notes">

Table 27.1 from Portney & Watkins, 2009

Sensitivity and specificity are not affected by prevalence. But prevalence plays a part in calculation of positive and negative predictive values.

</div>

***
### Measurement Validity 

![](../images/image-08-05.png)

<div class="notes">

+ Sim & Wright . 2000.

</div>

***
### Measurement 

+ Validity vs Reliability
	+ The chicken and the egg!

<div class="notes">



</div>

***
### Measurement 

![](../images/image-08-06.png)

<div class="notes">

Figure 6.1 from Portney & Watkins, 2009. Not valid, not reliable. Valid, not reliable. Not valid, reliable. Valid, reliable.

</div>

***
### Measurement 

+ Validity vs Reliability
	+ “ … a high degree of validity presupposes a high degree of reliability… ”
	+ “ … reliability does *not* presuppose. ”
	+ To establish reliability – only need to know where points are in relation to each other
	+ To establish validity – need to know where the “ target ” is in order to evaluate how close points are to this “ target ” 

<div class="notes">

Sumary. Always start by establishing reliability (where the points are in relation to each other).

</div>

***
### Measurement 

![](../images/image-08-07.png)

<div class="notes">

+ Validity vs Reliability (Table 9.1)

.footnote[Sim & Wright. 2000.]

</div>

***
### Assignment #7 

+ Generate a list of variables that you plan to include in your research proposal. Include in the list both dependent and independent variables. In the list include:
	+ Variables you will need to describe your sample,
	+ Variables you will need to control for in your analysis, and
	+ Variables you will need in order to test your RQ/RH.

<div class="notes">


RQ=research question, RH=research hypothesis.
</div>

***
`r read_text("hw09", fri[9])`

<div class="notes">



</div>

***
`r read_text("discussion09", fri[9])`

<div class="notes">



</div>

***

### Additional slides

### Additional issues to discuss

* Validity. http://www.pmean.com/02/validity.html
***
### Measurement 

+ What do we mean “ measurement ” ?
	+ assignment of numbers or symbols to the different levels or values of variables according to rules. ” 

<div class="notes">

We are talking about measurement this week. You are either taking stats or have taken it in the past. This is not a statistics class. Think about what type of variables you will collect and propose what statistical analysis.

Your book defines it as the assignment of numbers or symbols. Notice that this takes a quantitative approach. But even in a qualitative study, you still going to want to summarize information like demographics of your sample. Describe the quality or quantity of a variable.

There are qualitative or quantitative ways to measure things like pain.

We can also use measurements to make absolute decisions. Establish a cut-point. 

Conditions, distinctions.

</div>

***
### Measurement 

+ Assigning a number to represent …
	+ Continuous value
	+ Discrete value
+ Precision of measurement
	+ Continuous variable …
	+ Discrete variable …

<div class="notes">

Value that is assigned can take on any value. Preciseness is dependent on the device used to collect the measurement. It could be the precision of a laser versus a cruder measure. Precision should be appropriate to what you are trying to get at. Lack precision does not mean that your results are not continuous. If you round to the nearest inch, you still are measuring an underlying construct that can take on more precision. Example is blood pressure, interbeat interval, where rounding is done but it still represents a continuous variable.

Discrete values because of rounding, using counts, or a limited number of categories. As long as it represents an underlying construct, consider it as continuous.

</div>

***
### Measurement 

+ What is the measurement representing?
	+ Actual measurement …
		+ Length, time, …
	+ Indirect measurement
		+ Constructs
+ Whatever you are trying to measure ..
	+ Must be able to define it!

<div class="notes">

A survey or scale to get at depression, pain, those kind of things. These are abstract variables.

Bottom line is to define and operationalize how you measure something. It's easy to say something like quality of life but you have to pin this down and establish measurement validity. Consistency is important to avoid measurement bias.

Even for pretty obvious values like blood pressure, there are different ways to measure and you need to choose a single method for consistency and careful control, and when there are multiple methods, chose the method that is optimal for your needs. Blood pressure, for example, can be influenced by whether you are sitting or standing and whether you are talking while the measurement is being done.

Formalizing the rules for measurement helps reduce bias. This is a real concern when you have a variety of people doing the same measurements. How are new hires trained? Can you verify that they are trained to the standard that you have promised.

</div>

***
### Measurement 

+ Traditional levels (scales) of measurement
	+ Nominal
	+ Ordinal
	+ Interval
	+ Ratio

<div class="notes">

Nominal means names, classification, or categorical assignment. Names are mutually exclusive and exhaustive. Value that is shown is a number code or a text word. 

Ordinal is similar to nominal plus some orderliness as to what those responses mean. Example five point scale.

There is no true zero point. Can't perform arithmetic operatitons on ordinal variables.

You can talk about the direction of change.

Interval - difference between levels is consistent across the range of the scale.

Ratio. There is a true zero point.

</div>

***
### Measurement 

+ Authors categorization of levels of measurement
	+ Nominal
	+ Dichotomous
	+ Ordinal
	+ Normally distributed
+ Table 10.1

<div class="notes">

Gliner et al have a slightly different take. They talk about a normally distributed scale. It is hard to talk about the difference between ordinal and interval. 

Look at the literature in the area you work with. Several Likert scales summed to get a subscale.

</div>

***
### Measurement 

![](../images/image-08-01.png)

<div class="notes">

Table 10.1. This scale splits nominal into those with two levels (dichotomous, also known as binary) and more than two levels (nominal).

Remember how you coded nominal variables. You can (and should) use zero-one coding for dichotomous variables.

The average of a zero-one coded variable represents a probability.

Approximately normal scale requires at least five levels accorind to Gliner et al.

</div>

***
### Measurement 

![](../images/image-08-02.png)

<div class="notes">

Table 10.3 gives examples of the Gliner et al scales.

</div>

***
### Measurement 

+ Why does the scale of measurement matter?
	+ How it reflects your design and your research question
	+ How it determines the types of statistical analyses you will do
	+ How it defines what you can say about your results

<div class="notes">

The level of measurement comes from what you are trying to get at in your research.

Measurement scales can limit what you can say about your results.

</div>

***
### Assignment #6 

+ Prepare a brief paragraph that describes the research design you are using for your research proposal. This is the information that will probably appear in the Methods section of your proposal.

<div class="notes">

Go back are refer to Figure 4.1. Week 8 activities relate to the exemplar articles. These are on the Canvas site. There are six included. Pick one and get familiar with it.

</div>

***
`r read_text("hw08", fri[8])`

<div class="notes">



</div>

***
`r read_text("discussion08", fri[8])`

<div class="notes">



</div>

***

### Additional slides


### External Validity 

+ Generalizability
	+ Fig. 9.3
	+ Evaluating Research Validity Framework
	+ Two main aspects
		+ Population external validity
		+ Ecological external validity

<div class="notes">

The extent to which results can be generalized beyond this study. Extrapolate to the bigger world.

Evaluating freamework (questions 14 and 15). 

Population: how participants were selected.

Ecologic validity: the more control you have, the better the internal validity but the less like it is for real world. The naturalness of the setting, rapport.

Figure 9.3. 

</div>


### External Validity 

+ Population external validity
	+ How participants were selected for the study?
	+ Is sample representative of the target population?
	+ Validity framework

<div class="notes">



</div>


### External Validity 

+ Ecological external validity
	+ Whether the results can be generalized to real-life outcomes
	+ Trade-off with control of study
	+ Validity framework

<div class="notes">



</div>


### External Validity 

![](../images/image-07-07.png)

<div class="notes">



</div>


### Sampling and Validity 

![](../images/image-07-08.png)

<div class="notes">

Figure 9.4. External validity is influenced by the sampling process. Internal validity is influenced by the allocation of this sample to treatment groups.

</div>

