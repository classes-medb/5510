---
title: "Video 13 - Statistical models"
author: "Steve Simon"
output: 
  powerpoint_presentation:
    reference_doc: ../doc/template.pptx
    slide_level: 3
---

```{r echo=FALSE}
source("prelims.R", echo=FALSE)
```

### Measurement 

+ Traditional levels (scales) of measurement
	+ Nominal
	+ Ordinal
	+ Interval
	+ Ratio

+ Special cases
  + Binary data
  + Count data, rate data
  + Time-to-event

<div class="notes">

Measurement scales are an important, but controversial, categorization of the types of variables used in research. They help you decide what statistics are appropriate for your data. I don't want to get to deeply into this, but I do want to give you enough context so that you can write a decent data analysis plan for your methods section.

There are four categories: nominal, ordinal, interval, and ratio, that were first described by a psychologist, S.S. Stevens, over 80 years ago.

Nominal data is categorical, meaning a small number of values. An additional requirement for nominal data is that order is not important. If you can re-order the list of categories without causing too much fuss, you have nominal data. Examples of nominal data include race, gender, and geographic region.

Ordinal data is also categorical, but you cannot re-order the list without changing things. Order is important. All Likert scale items are ordinal. Military ranks are ordinal. Education levels are ordinal, more or less, but if you make distinctions along the level of M.D. versus Ph.D., it is not. Use a bit of common sense. It works just fine for High school, some college, two year degree, four year degree, graduate/professional degree. Stages of cancer is another example.

Interval data and ratio data are two types of continuous data. Don't worry too much about the distinction between interval and ratio. It is not that important. The very important distinction is between ordinal data and interval data.

For an interval scale, the difference of one unit means the same thing when you are on the low end versus the high end of the scale. Examples of interval scales include temperature and IQ tests.

for a ratio scale, the ratio of two measurements means the same thing when you are on the low end versus the high end of the scale. Concepts such as "twice as much" are meaningful for ratio data. Ratio data also have a fixed zero point. Examples of ratio scales include birth weight, age, income.

There are three special cases worth describing.

Binary data is data that only has two possible values. Examples include yes/no responses, gender (male/female, assuming that transgender is not an issue), and correct/incorrect responses on a test. Don't bother making a distinction between nominal or ordinal data if your variable is binary. There is no way to re-order the data if there are only two values, except for the trivial case of reversing the two levels. It is best to treat binary data as a subset of nominal data.

Count data is a type of ratio data, but it is a bit unsual because it cannot include fractional or decimal values. It is also bounded below by zero. For the most part, you can treat count data as ratio data, except that linear regression does not work well for count data. Count data almost always violates the linear regression assumptions of normality and homogeneity of variance.

Time-to-event data is also a type of ratio data, but it is also a bit unusual. Time-to-event data almost always has censored values, values where the event has not occurred yet. It also is bounded below by zero. It also tends to violate the same assumptions in linear regression: normality and homogeneity.

A neagtive requirement for ordinal data is that sums computed on an ordinal scale are meaningless. You can't talk about how a private plus a sergeant is equal to two corporals, even though you might code private=1, corporal=2, sergeant=3.

</div>

### ordinal verus interval controversy
+ Example: grade point average
  + Two B's equal and A plus a C?
  + Shift from A to B versus a shift from D to F?
+ Likert scale
  + Is an average meaningful?
  + Independent variable for linear regression?
  + Dependent variable for linear regression?
+ Is a sum of Likert scale items different?
+ Purist versus pragmatist

<div class="notes">



</div>

### Permissible statistical summaries

+ Nominal: percentage, mode
+ Ordinal: median
+ Interval: mean, standard deviation
+ Ratio: Coefficient of variation
+ Special cases

<div class="notes">

For nominal data, there are very few numeric summaries that are meaningful. You can't compute a standard deviation for gender or an average race. About the only meaningful numeric summary for nominal data is percentages. You could compute a mode, the most frequently occurring value, but this is not used all that often.

For ordinal data, you can compute the median in addtion to percentages. You might also compute other percentiles like the quartiles. But an average is not a good option here.

You can, however, compute an average for interval data, as well as a standard deviation.

The ratio scale allows you to calculate one additional statistic, the coefficient of variance, but this is hardly worth mentioning. The coefficient of variance, if you are curious, is computed as the standard deviation divided by the mean. It is a unitless measure of spread, which is helpful at times. But quite honestly, there is almost no practical difference between interval data and ratio data.

There is a natural hierarchy to these measurements in that any statistic that can be used to summarize nominal data can also be used to summarize ordinal, interval, or ratio data. Any statistic that can be used to summarize ordinal data can also be used to summarize interval or ratio data. Finally, any statistic that can be used to summarize interval data can also be used to summarize ratio data.

</div>

### Permissible models

+ Special cases
  + Binary: Logistic regression
  + Counts: Poisson regression
  + Time-to-event data: Cox proportional hazards regression
+ Nominal: Chi-square tests
+ Ordinal: Non-parametric tests
+ Interval/ratio: linear regression

<div class="notes">

In addition to the permissible statistical summaries, there are permissible statistical models. Let's deal with the special cases first.

</div>

### First break

+ What have you learned?
  + Scales of measurement
  + Ordinal verus interval controversy
+ What's coming next?
  + Descriptive statistics
  + Linear regression

### Descriptive statistics
+ Part of every quantitative study
+ Table 1, overall summaries
  + Outcomes and covariates
  + Means and standard deviations
  + Percentages (always show denominator)
+ Key subgroup comparisons
  + Crosstabulations
  + Means/standard deviations by subgroup

<div class="notes">

Your descriptive statistics should be a part of any research study. This is done in almost every research paper and it almost always appears as the first table in your paper. So we sometimes call these table 1 statistics.

Your descriptive statistics should include your outcome variables, most certainly, but also your covariates. Covariates are variables that are not of direct interest but which can potentially influence your outcome anyway. This includes, in any human study, things like age and gender.

if your data are continuous (interval or ratio scale), summarize your data with a mean and standard deviation. Some people advocate a different summary if your data is skewed, using the median and the range, for example, instead of the mean and standard deviation. There is no consensus on how much the data has to be skewed, or even if you need to switch at all. I, for one, see the mean and standard deviation as useful summaries, even for skewed data, but I am in a minority on this.

For categorical outcomes summarize using a percentage. Always show what your numerator and denominator would be if you report percentages.

If you want to compare across key demographic groups, use crosstabulations for categorical outcomes and means/standard deviations by subgroup for continuous outcomes.

Let me talk a bit about crosstabulations because these are almost always done poorly.

</div>

### Statistical Analysis Selection 

+ Parametric statistics – assumptions
	+ DV values come from a population of values that is normally distributed (normality)
	+ Variances of the groups are equal (homogeneity of variance)
		+ Unequal group sizes and unequal variances <U+F0E8> increased Type I errors
	+ All participants with a particular group must be independent of each other (independence)
		+ Also an assumption for nonparametric statistics

<div class="notes">



</div>

### Data Analysis 

+ What do you need to accomplish with your data analysis?
	+ Quality check of data
	+ Description of sample
	+ Test of hypotheses/research questions
	+ Additional exploratory analyses

<div class="notes">



</div>

***
### Data Analysis 

+ Quality check of data
	+ Completeness of data collection
	+ Review for responses that are ambiguous, out of range, etc
	+ “ Edit ” responses as needed
	+ Check response frequencies
		+ Discover need for recoding
		+ Scale of measurement
	+ Check subscale coding and scores
		+ Check inter-item reliability

<div class="notes">



</div>

### Rules for crosstabulations

+ Never display multiple statistics
+ Place treatment/exposure categories in the rows
+ Summarize using row percentages
+ Many rows, not many columns
+ Round liberally.

<div class="notes">

When you are deciding how to display two by two (or larger) tables, you have a variety of ways to do this. No way is correct all the time, and some of choices reflect subjective judgment. But here are some rules I use.

Never display more than one type of number in a table. Statistical software like SPSS can produce counts, row percents, column percents, cell percents, expected counts, residuals, and/or cell contribution to chi-squared values. At one time or another you might want to use each of these statistics, but never all at one time. Two or more numbers in a table causes confusion and makes your tables harder to interpret.

Present a single summary statistic in the table if at all possible. If you need to display two summary statistics (for example, both counts and row percentages), then place the counts in one table and the row percentages in a different table. If you have to fit them in the same table, place the two numbers side by side with the less important number appearing second and in parentheses For example, 54% (257).

Row percentages are usually best. Row percentages are the percentages you compute by dividing each count by the row total. Row percentages place the comparison between two numbers within a single column, so that one number is directly beneath the number you want to compare it to. This is usually better than column percents, where the numbers you want to compare are side by side. If you find that column percentages make more sense. Consider swapping the rows and columns.

If you find that cell percentages make the most sense, consider creating composite categories that combine the row and column categories. Cell percentages are the percentages that you get when you divide each cell count by the overall total. When cell percents are interesting, it usually means that you are interested in the four distinct categories in your two by two table. For example, you are interested in seeing what fraction of job candidates are white males, rather than seeing how the probability of being male influences the probability of being white. For this type of data, treat it as a single categorical variable with four levels (white males, white females, black males, black females) rather than two categorical variables with each having two levels (black/white, male/female).

Place the treatment/exposure variable as rows and outcome variable as columns. This relates to the above item. You usually are interested in the probability of an outcome like death or disease, and you are interested in how this probability changes when the treatment or exposure changes. Arranging the table thusly and using row percents usually gets you the comparison you are interested in.

If one variable has a lot more levels than the other variable, place that variable in rows. A table that is tall and thin is usually easier to read than a table that is short and wide. It is easier to scroll up and down rather than left and right. For a really large number of levels, you might have to print your table on two or more pages. Usually it is a lot easier to align these pages if the table is tall and thin. A short wide table that is split on two or more pages is often a disaster.

Whenever you report percentages, always round. A change on the order of tenths of a percent are almost never interesting or important. Displaying that tenth of a percent makes it harder to manipulate the numbers to see the big picture.

Don't worry about whether your percentages add up to 99% or 101%. First of all, it can't happen with a two by two table unless you round incorrectly. For a larger table, it can happen, but your audience is sophisticated enough to understand why this is the case. No one, for example, is going to be upset when 33% plus 33% plus 33% adds up to less than 100%.

When in doubt, write out your table several different ways. Pick out the one that gives the clearest picture of what is really happening. Don't rely on the first draft of your table, just like you would never rely on the first draft of your writing.

</div>
### Table of percentages

![](../images/12/percentage2.gif)

<div class="notes">

A simple fictitious example will help illustrate these points.

We classify people by their income (rich/poor) and also by their attitude (happy/miserable). There are, for example,  30 rich happy people in our sample and 70 poor miserable people.

</div>
  
### Table of column percentages

![](../images/12/percentage1.gif)

<div class="notes">

This figure shows column percentages. We compute this by dividing each number by the column total.

We see for example that only 25% of all happy people are rich. This is a conditional probability and is usually written as P[Rich | Happy]. Read the vertical bar as "given." So this probability is read as the probability of being rich given that you are happy.

</div>
  
### Table of row percentages

![](../images/12/percentage3.gif)

<div class="notes">

This figure shows row percentages. We compute this by dividing each number by the row total.

We see, for example that 75% of rich people are happy. This is a different conditional probability, P[Happy | Rich]. Read this as the probability of being happy given that you are rich.

Notice the distinction between the two probabilities. Only a few happy people are rich, but most rich people are happy.

</div>
  
### Table of cell percentages

![](../images/12/percentage4.gif)

<div class="notes">

This figure shows cell percentages. We compute this by dividing each number by the grand total. Each percentage represents the probability of having two conditions. For example, there is a 15% chance of being rich and happy.

</div>
  
### Combining two numbers

![](../images/12/percentage5.gif)

<div class="notes">

The table above shows a good format for combining two numbers in a single table.

</div>
  
### Table of percentages

![](../images/12/percentage6.gif)

<div class="notes">

This is an alternate way of displaying cell percentages.

If we had a six categories for attitude rather than just two, we might arrange the table differently.

</div>
  
### Table of percentages

![](../images/12/percentage7.gif)

<div class="notes">

Notice that this table would not require any sideways scrolling.

</div>

### Rules for crosstabulations

+ Never display multiple statistics
+ Place treatment/exposure categories in the rows
+ Summarize using row percentages
+ Many rows, not many columns
+ Round liberally.

<div class="notes">

Here are those rules again.

Never display more than one type of number in a table.

Place the treatment/exposure variable as rows and outcome variable as columns.

Row percentages are usually best.

If one variable has a lot more levels than the other variable, place that variable in rows.

Whenever you report percentages, always round.

Don't worry about whether your percentages add up to 99% or 101%.

When in doubt, write out your table several different ways.

</div>

### Graphs
+ Overall summaries
  + Histograms for continuous data
  + Bar/pie charts for categorical data
+ Assessing relationships
  + Side by side pie/bar charts
  + Boxplots
  + Scatterplots
  
### Histogram examples (1 of 3)

![Histogram showing a roughly bell shaped curve](../images/12/hist1.gif)

### Histogram examples (2 of 3)

![Histogram showing a skewed right distribution](../images/12/hist2.gif)

### Histogram examples (3 of 3)

![Histogram showing a bimodal distribution](../images/12/hist3.gif)

### Side by side pie/bar charts

+ Pies and bars only work well for 2 or 3 categories
  + Pacman charts
+ No good graphs for more categories
+ Avoid cheap 3D effects

### Boxplot

![](../images/12/boxplot2.gif)

<div class="notes">

The box plot is a graphical display of a five number summary. Sometimes the box plot is also known as a box and whiskers plot. It is very useful for examining relationships between a categorical variable and a continuous variable.

Here are the four steps you follow to draw a boxplot.

Draw a box from the 25th to the 75th percentile.

Split the box with a line at the median.

Draw a thin lines (whisker) from the 75th percentile up to the maximum value.

Draw another thin line from the 25th percentile down to the minimum value.

The length of the box in a box plot, i.e., the distance between the 25th and 75th percentiles, is known as the interquartile range. You can use this box length to detect outliers. If any whisker is more than 1.5 times as long as the length of the box, then we have evidence of outliers. A common variation on the box plot is to draw the whisker to the value which is just shy of 1.5 box lengths away, and highlight each individual data point more than 1.5 box lengths away.

The boxplot is useful for comparing the distributions of two different groups. If the median in one box exceeds the end of the box of the other group, that is evidence of a "large" discrepancy between the two groups. What passes as the median for one group would actually be the 25th or 75th percentile of the other group.

Just about any statistical software program (SAS, SPSS, Stata, R, etc.) can produce boxplots and you can find code to produce boxplots in many programming languages.

Notice that these boxplots are horizontal rather than vertical. The main reason to turn these 90 degrees is if your labels and axes fit better this way.

</div>

### Scatterplot

![](../images/12/scatter1.png)

<div class="notes">

This is a scatterplot, a useful way to summarize the association between two continuous variables.

Often a trend line or a smoothing curve helps. This seems to be the case when you have a lot of data, meaning a lot of overprinting, and the data is fairly noisy.

</div>

### Quality checks (internal use, mostly)

+ Minimum and maximum checks
  + Out of range
  + Zero variation
+ Missing value count
+ List five five rows, last five rows
+ Correlations

<div class="notes">

There are some descriptive statistics that you should run just for your own benefit. Check the minimum and maximum values for every numeric variable in your data set. I received a data set with a bunch of 5 point Likert scale items and for most of the variables, the minimum was 1, but the maximum was 33 or 55. Guess what happened?

Get a missing value count and monitor it closely. See if there are odd patterns, such as the missing values declining over time but then jumping back up at the end. It could be normal, but it could be an indication of a problem.

When you get correlations, look for unexpected patterns, such as a negative correlation where a positive one was expected. It could be okay, or it could mean that the scales on a Likert scale got switched around halfway through the survey due to a misprint.

</div>

### Data reduction

* Check composite scores
  + Check Cronbach's alpha
  + Examine leaving out single items
  + Factor analysis, Structural Equations Modeling

### Data transformations

* Ideal - selected a priori
  + Sometimes based on precedent
  + Sometimes motivated by theory
  + Sometimes based on empirical findings
+ Log transformation
+ Collapse low frequency categories
  
### Linear regression

+ High school algebra
  + Y = m X + b
  + m = Δy / Δx
+ The slope represents the estimated average change in Y when X increases by one unit.
+ The intercept represents the estimated average value of Y when X equals zero.

<div class="notes">

When I ask most people to remember their high school algebra class, I get a mixture of reactions. Most recoil in horror. About one in every four people say they liked that class. Personally, I thought that algebra, and all the other math classes I took were great because they didn't require writing a term paper.

One formula in algebra that most people can recall is the formula for a straight line. Actually, there are several different formulas, but the one that most people cite is

Y = m X + b

where m represents the slope, and b represents the y-intercept (we'll call it just the intercept here). They can also sometimes remember the formula for the slope:

m = Δy / Δx

In English, we would say that this is the change in y divided by the change in x.

In linear regression, we use a straight linear to estimate a trend in data. We can't always draw a straight line that passes through every data point, but we can find a line that "comes close" to most of the data. This line is an estimate, and we interpret the slope and the intercept of this line as follows:
    
The slope represents the estimated average change in Y when X increases by one unit.

The intercept represents the estimated average value of Y when X equals zero.

Be cautious with your interpretation of the intercept. Sometimes the value X=0 is impossible, implausible, or represents a dangerous extrapolation outside the range of the data.

</div>

### Age vs duration

![](../images/12/linear1.gif)

<div class="notes">

The graph shown below represents the relationship between mother's age and the duration of breast feeding in a research study on breast feeding in pre-term infants.

The regression coefficients are shown below. The intercept, 6, is represented the estimated average duration of breast feeding for a mother that is zero years old. This is an impossible value, so the interpretation is not useful. What is useful, is the interpretation of the slope.

Notice that a 20 year old mother has a duration of 13 weeks. A 40 year old mother has a duration of 21 weeks.

Calculate 21-13 divided by 40-20 equals 8 divided by 20 equals 0.4. The estimated average duration of breast feeding increases by 0.4 weeks for every extra year in the mother's age.

</div>

### Age vs duration

![](../images/12/linear2.gif)



### Treatment vs duration

![](../images/12/linear3.gif)

<div class="notes">

When X is categorical, the interpretation changes somewhat. Let's look at the simplest situation, a binary variable. A binary variable can have only two possible categories. Some examples are live/dead, treatment/control, diseased/healthy, male/female. We need to assign number codes to the categories. Most people assign the codes 1 and 2, but it is actually better to assign the codes 0 and 1.

In a study of breast feeding, we have a treatment group and a control group. Let us label the treatment group as 1 and the control group as 0. The outcome variable is the age when breast feeding stopped.

In this situation, the intercept, 13, represents the average duration for the control group. 

Calculate 20-13 divided by 1-0.

The slope is 7, which is the change in the average duration when we move from the control group to the treatment group.

When we represent a binary variable using 0-1 coding, the slope represents the estimated average change in Y when you switch from one group to the other.

The intercept represents the estimated average value of Y for the group coded as zero. 

</div>

### Treatment vs duration

![](../images/12/linear6.gif)



### Adjusted

![](../images/12/linear12.gif)

<div class="notes">

There are two types of models, crude models and adjusted models. A crude model looks at how a single factor affects your outcome measure and ignores potential covariates. An adjusted model incorporates these potential covariaties. Start with a crude model. It's simpler and it helps you to get a quick overview of how things are panning out. Then continue by making adjustments for important confounders.

A crude model for comparing duration of breast feeding to feeding group would be a t-test. I prefer, however, to present a general linear model because it provides a unifying framework for diverse statistical methods like analysis of variance, analysis of covariance, multiple linear regression, repeated measures designs, and t-tests.

Shown below is the table of tests from the general linear model procedure.

The general linear model uses an F test instead of the t test, but in this context, these two tests are mathematically equivalent. The p-value for comparing feeding groups is .001, which indicates a significant difference between the two groups.

The general linear model also has a table of estimates, which is presented below.

The intercept represents the average duration of breast feeding for the NG tube group. We see that the average duration is 20 weeks for the NG tube group. The (FEED_TYP=1) term is an estimate of how much the average duration changes when we move from the NG tube group to the bottle group. We see that the bottle group has an average duration that is 7 weeks shorter.

Shown below is a table of means from the general linear model.

We see that the difference between the two means is roughly 7 weeks, which confirms the results shown previously.

The previous model was a crude model. We see a seven week difference between the two groups, but could some of all of this difference be due to the fact that the NG tube group had older mothers? To answer this, we need to fit an adjusted model.

Shown below is the table of tests for a general linear model that includes mother's age in the model.

The p-value for feeding group is .009, which is still significant, even after adjusting for the effect of mother's age.

Shown below is the table of estimates from the same general linear model.

This table shows that the effect of bottle feeding is to decrease duration of breast feeding by about six weeks, after adjusting for mother's age. Each year that a mother is older increase the duration of breast feeding by a quarter of a week.

A previous descriptive analysis of this data revealed that the average age for mothers in the treatment group is 29 years and the average age for mothers in the control group is 25 years. When you see a discrepancy like this in an important covariate, you need to assess whether the four year gap in average ages could account for part or all of the effect of the treatment group.

This analysis shows that the four year gap only accounts for a small portion of the difference. Since each year of age changes the duration by a quarter week, this means that the difference between mother's ages acounts for just one week in the 7 week difference we saw in the crude model.

Shown below is the table of means.

This table now adjusts for mother's age. The mean for the bottle fed group is adjusted upward to what it would be if the average age of the mothers in this group were 27 rather than 25. The mean for the NG tube group is adjusted downward to what it would be if the average age were 27 instead of 29. Note that the adjusted mean duration is half a week higher than the crude mean duration in the bottle group and that the adjusted mean duration is half a week lower than the crude mean duration for the NG tube group. This confirms that the difference between the two feeding groups is roughly 6 weeks, after adjusting for mother's age. This is one week less than the crude model.

This is not the final model. We should examine the effect of delivery type and account for the fact that we have some data on twins. I hope, though, that this presentation gives you a general idea of what crude and adjusted models are.


</div>

### Second break

+ What have you learned?
  + Descriptive statistics
  + Linear regression
+ What's coming next?
  + Logistic regression
  + Poisson regression
  
### Third break

+ What have you learned?
  + Logistic regression
  + Poisson regression
+ What's coming next?
  + Cox regression
  
### Fourth break

+ What have you learned?
  + Cox regression
+ What's coming next?
  + Qualitative data analysis

### Analysis of Qualitative Data-resources

+ Typically, a one-hour interview requires a minimum of three to four hours (or more) of analysis.
+ Involve the participants in the process, especially for narrative research.
+ Tools:
  + focus groups
  + semi-structured interviews
  + participant observation
  + archival records

### Tools


### Inductive process
+ Start with the specific (raw data / transcript)
	+ Develop a theoretical framework from the data
	+ Conceptual categories emerge from the data
	+ Iterative process
+ Define the process
  + Who does the work
  + Privacy protections
  + How you will adapt

<div class="notes">

Quantitative analysis is almost always structured and defined in detail prior to data collection. But qualitative analysis is different. You are trying to build a theoretical frame work from data, so you are moving from specific data to a generality. This is known as inductive research. You are using small pieces of text to develop broad themes and establish an overarching pattern or framework.

You data collection may change as you collect information. I was helping a group of nurses who were running some focus groups (I didn't do too much other than observe from a distance). A big pattern emerged with the very first focus group. There was a generational difference in norms and expectations. So much so that the first focus group almost devolved into a shouting match. So the nurses decided to ask lots of questions about generational issues and they also decided wisely to separate the boomer generation from the Xers.

So if things can and should change as you collect data, how do you write a methods section? You have to focus on the process. Well there are small things, such as who does the questions in the interviews, how the interviews will be recorded and transcibed, and so forth. You should mention privacy protections.

Most importantly, if you expect to let the early data drive how you will collect later data collection efforts, say this explicitly and explain what you will be looking for as you decide how and when to make mid-course adaptations.

</div>

### Analysis process for qualitative data
+ Your research question is only your starting point.
+ Don't let your question blind you to new information
+ Build themes before you complete your data collection
	+ Check back against the  raw data
	+ Look for negative examples
	+ Don't ignore infrequently voiced themes
+ When have you achieved saturation?

<div class="notes">

Software allows you to go back and forth between themes and the original text. Cross-referencing is important. Find examples easily. It helps provide validity checks. Does someone agree with your categorization.

Categories need to be faithful to the data. This is internal validity. But you need to place this in a broader understanding.

You need to be careful about trying to quantify this too much. Keep in mind that just because someone talks about something a lot does not mean that this is the most important thing to them. The most important thing might be something that people are uncomfortable talking about.

Phrasing of the person versus observations generated by the researcher. Labels should be considered as provisional as you work with more complete information. It is an iterative process. Go back to original data. You're looking at the specific of what's been told versus a broader understanding.

Establishing the criteria may depend on how many people are doing the coding. If you don't have a pre-existing structure, the coding may evolve and may involve 100% review (both raters review everything and resolve discrepancy). Then a third person does an audit.

The point at which you are not gaining any new insight, no new categories being identified, no new relationships being defined. Saturation is a way to assess your sample size. Have you gotten a representative amount of data. 

Really important to keep notes of the coding and analysis process, so you can check your work. Allows you to do qualirty checks on the process.

</div>


### Role of Judgment
+ Balancing act 
	+ Level of creativity by coder to identify categories/relationships
	+ Must reflect the informants thoughts
	+ Audit of the coding by an independent person can check for the match between the coding and the source information
+ Look for "negative cases"

<div class="notes">

</div>

### Tenth break

+ What have you learned?
  + Qualitative data analysis
+ What's coming next?
  + Writing a methods section
  
### What purpose does a methods section serve?
+ Assessment of the quality of your research
  + Brag here about your rigor
  + Save limitations for discussion
+ Allow others to replicate

### What should not be included in the methods section

+ "The Methods section should include only information that was available at the time the plan or protocol for the study was being written; all information obtained during the study belongs in the Results section."
  + International Commitee of Medical Journal Editors. Uniform requirements for manuscripts submitted to biomedical journals: Writing and editing for biomedical publication. J Pharmacol Pharmacother. 2010;1(1):42–58.

### What belongs in the methods section
+ Every methods section is different
+ General structure
  + Participants
  + Materials
  + Procedures
  + Measures
  + Analysis
  
### Participants
+ Where you will find your participants
+ Inclusion/exclusion criteria
+ Efforts to insure representativeness

### Materials/Procedures
+ Only document the non-routine
+ Materials
  + Chemicals
  + Include company and location
+ Procedures
  + Running complex equipment
  + Multiple step laboratory methods

### Measures
+ Outcome variables
+ Independent variables
+ Covariates
+ Validity/reliability


### Analysis
+ Research hypotheses / questions
+ Sample size justification
+ Descriptive methods (see boilerplate)
  + Boilerplate: "Continuous variables were summarized as means and SDs, and categorical variables were summarized as percentages." Saleem 2019.
+ Statistical model
+ Adjustments for multiplicity
+ Handling missing values/dropout
+ Alpha level and one/two sided tests
  + Boilerplate: "All tests were two sided, and P values below the 5% level were regarded as significant." Lokken 1995.

<div class="notes">

Boilerplate refers to text that is used so often and without any changes that you can safely use it in your publication without fear of being accused of plagiarism. The first boilerplate comes early in your data analysis methods, and describes how you are summarizing continuous variables using means and standard deviations and summarizing categorical variables using percentages. The second boilerplate mentions that all of your statistical tests are two sided (which they are 90% of the time) and that you use an alpha level of 0.05 to decide what is statistically significant (which you do 99.99% of the time).

Between-Groups Designs: Each person "... is in one and only one condition or group."
Within-Subjects (Repeated Measures) Designs: Each person "...receives or experiences all of the conditions or levels of the independent variable"

Reason to use and not use Within-Subjects design?

</div>

### Sections of a methods section
+ Participants, Procedure, Measures/Materials, Analysis section (Frye, no date)

### Sections of a methods section
+ Subjects, Ethical considerations, Preparations, Protocol design, Measurements and calculations, Data analysis (Kallet 2004)

### Sections of a methods section
+ Participants, Materials (Stimuli, Testing materials, Background questionnaire), Procedure (Tasks, Design and analyses)

### Sections of a methods section
+ What materials did you use? Who were the subjects of your study? What was the design of your research? What procedure did you follow? (Kallestinova 2011)

### Sections of a methods section
+ Selection and description of participants, Technical information, Statistics. International Commitee of Medical Journal Editors 2010.

### Sections of a methods section
+ Materials (Chemical, Experimental materials, Experimental animals, Human subjects), Methods (Study design, Measurements/assessments, Statistical analyses) (Ghasemi 2019)

### Conclusion

+ Scales of measurement
+ Descriptive statistics
+ Linear, logistic, Poisson, and Cox regression
+ Analysis of qualitative data
+ Writing a methods section

